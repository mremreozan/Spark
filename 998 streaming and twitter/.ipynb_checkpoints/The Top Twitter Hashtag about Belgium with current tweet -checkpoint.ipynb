{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext          \n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10) # we create a RDD every 10 seconds\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"localhost\", 5555)\n",
    "# we learn port number which we listen by working TweetRead.py in command line. must look the line of scc.start() for details.\n",
    "# write tweet app info to TweetRead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window(20) # we fill the RDD every 20 seconds again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\")\n",
    "Tweet = namedtuple('Tweet', fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "( lines.flatMap(lambda text: text.split(\" \"))\n",
    " .filter(lambda words: words.lower().startswith(\"#\"))\n",
    " .map(lambda words: (words.lower(), 1))\n",
    " .reduceByKey(lambda a, b: a+b)\n",
    " .map(lambda rec: Tweet(rec[0], rec[1]))\n",
    " .foreachRDD(lambda rdd: rdd.toDF().sort(desc(\"count\"))\n",
    "             .limit(10).registerTempTable(\"tweets\"))\n",
    ")\n",
    "\n",
    "# we select only words starting with '#' from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x2c396d4c400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start() \n",
    "# Before we don't work this line, we must learn by working TweetRead.py(python TweetRead.py) in command line.\n",
    "# TweetRead.py bring tweets to us aboout 'belgium' which is defined in the TweetRead.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1722.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 50.0 failed 1 times, most recent failure: Lost task 1.0 in stage 50.0 (TID 36, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f23ff5d2d25c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtop_10_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Select tag, count from tweets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtop_10_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_10_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2142\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \"\"\"\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1722.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 50.0 failed 1 times, most recent failure: Lost task 1.0 in stage 50.0 (TID 36, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "count = 0\n",
    "while count < 10:\n",
    "    time.sleep(3)\n",
    "    top_10_tweets = sqlContext.sql(\"Select tag, count from tweets\")\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    sns.barplot(x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.title('The Top Twitter Hashtag about Belgium in '+ str(today))\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
